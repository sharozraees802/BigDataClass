{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tLoad data from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pySp1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case of error in type casting to datetime, use following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('D:/DataSets/emp.csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tDisplay the schema of the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['empno', 'ename', 'job', 'mgr', 'hiredate', 'sal', 'comm', 'deptno']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_check = df.columns\n",
    "cols_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empno', 'string'),\n",
       " ('ename', 'string'),\n",
       " ('job', 'string'),\n",
       " ('mgr', 'string'),\n",
       " ('hiredate', 'string'),\n",
       " ('sal', 'string'),\n",
       " ('comm', 'string'),\n",
       " ('deptno', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: string (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: string (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      " |-- comm: string (nullable = true)\n",
      " |-- deptno: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7839|  KING|PRESIDENT|null|11/17/2001|5000|null|    10|\n",
      "| 7698| BLAKE|  MANAGER|7839|05/01/2001|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|06/09/2001|2450|null|    10|\n",
      "| 7566| JONES|  MANAGER|7839|04/02/2001|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|09/28/2001|1250|1400|    30|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date, unix_timestamp, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"empno\", df.empno.cast('integer')) \\\n",
    "        .withColumn(\"mgr\", df.mgr.cast('integer')) \\\n",
    "        .withColumn(\"sal\", df.sal.cast('float')) \\\n",
    "        .withColumn(\"comm\", df.comm.cast('float')) \\\n",
    "        .withColumn(\"deptno\", df.deptno.cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('hiredate', to_date(unix_timestamp(df.hiredate, 'MM/dd/yyyy').cast(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empno', 'int'),\n",
       " ('ename', 'string'),\n",
       " ('job', 'string'),\n",
       " ('mgr', 'int'),\n",
       " ('hiredate', 'date'),\n",
       " ('sal', 'float'),\n",
       " ('comm', 'float'),\n",
       " ('deptno', 'int')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show the head of the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+------+------+\n",
      "|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+---------+----+----------+------+------+------+\n",
      "| 7839|  KING|PRESIDENT|null|2001-11-17|5000.0|  null|    10|\n",
      "| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0|  null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0|  null|    10|\n",
      "| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0|  null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|\n",
      "+-----+------+---------+----+----------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate=datetime.date(2001, 11, 17), sal=5000.0, comm=None, deptno=10),\n",
       " Row(empno=7698, ename='BLAKE', job='MANAGER', mgr=7839, hiredate=datetime.date(2001, 5, 1), sal=2850.0, comm=None, deptno=30),\n",
       " Row(empno=7782, ename='CLARK', job='MANAGER', mgr=7839, hiredate=datetime.date(2001, 6, 9), sal=2450.0, comm=None, deptno=10),\n",
       " Row(empno=7566, ename='JONES', job='MANAGER', mgr=7839, hiredate=datetime.date(2001, 4, 2), sal=2975.0, comm=None, deptno=20),\n",
       " Row(empno=7654, ename='MARTIN', job='SALESMAN', mgr=7698, hiredate=datetime.date(2001, 9, 28), sal=1250.0, comm=1400.0, deptno=30)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pyspark, take() and show() are different. show() prints results, take() returns a list of rows (in PySpark) and can be used to create a new dataframe. They are both actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tSelect Columns from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| ename|   sal|\n",
      "+------+------+\n",
      "|  KING|5000.0|\n",
      "| BLAKE|2850.0|\n",
      "| CLARK|2450.0|\n",
      "| JONES|2975.0|\n",
      "|MARTIN|1250.0|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ename','sal').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative\n",
    "df1 = df.select('ename','sal')\n",
    "<p>\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tShow the Statistics of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|summary|ename|               sal|\n",
      "+-------+-----+------------------+\n",
      "|  count|   14|                14|\n",
      "|   mean| null| 2073.214285714286|\n",
      "| stddev| null|1182.5032235162716|\n",
      "|    min|ADAMS|             800.0|\n",
      "|    max| WARD|            5000.0|\n",
      "+-------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ename','sal').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|summary|     job|\n",
      "+-------+--------+\n",
      "|  count|      14|\n",
      "|   mean|    null|\n",
      "| stddev|    null|\n",
      "|    min| ANALYST|\n",
      "|    max|SALESMAN|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('job').show() # show the stats for a specific column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Drop Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      job|\n",
      "+---------+\n",
      "|PRESIDENT|\n",
      "|  MANAGER|\n",
      "|  MANAGER|\n",
      "|  MANAGER|\n",
      "| SALESMAN|\n",
      "| SALESMAN|\n",
      "| SALESMAN|\n",
      "|    CLERK|\n",
      "| SALESMAN|\n",
      "|  ANALYST|\n",
      "|    CLERK|\n",
      "|  ANALYST|\n",
      "|    CLERK|\n",
      "|    CLERK|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      job|\n",
      "+---------+\n",
      "|  ANALYST|\n",
      "| SALESMAN|\n",
      "|    CLERK|\n",
      "|  MANAGER|\n",
      "|PRESIDENT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('job').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      job|\n",
      "+---------+\n",
      "|  ANALYST|\n",
      "| SALESMAN|\n",
      "|    CLERK|\n",
      "|  MANAGER|\n",
      "|PRESIDENT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('job').dropDuplicates().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job = df.select('job').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      job|\n",
      "+---------+\n",
      "|  ANALYST|\n",
      "| SALESMAN|\n",
      "|    CLERK|\n",
      "|  MANAGER|\n",
      "|PRESIDENT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_job.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+---+--------+---+----+------+\n",
      "|empno|ename|job|mgr|hiredate|sal|comm|deptno|\n",
      "+-----+-----+---+---+--------+---+----+------+\n",
      "|    0|    0|  0|  1|       0|  0|  10|     0|\n",
      "+-----+-----+---+---+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## See if we have missing values\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark.sql.functions.isnan(col): an expression that returns true iff the column is NaN.\n",
    "<br>\n",
    "isNull() :True if the current expression is null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Drop NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "df2 = df.dropna(how='any', subset =['comm', 'mgr'])\n",
    "print(df2.count())\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "df02 = df.dropna(how='all', subset =['comm', 'mgr'])\n",
    "print(df02.count())\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+------+------+\n",
      "|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+---------+----+----------+------+------+------+\n",
      "| 7839|  KING|PRESIDENT|null|2001-11-17|5000.0|  null|    10|\n",
      "| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0|  null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0|  null|    10|\n",
      "| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0|  null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|\n",
      "| 7499| ALLEN| SALESMAN|7698|2001-02-20|1600.0| 300.0|    30|\n",
      "| 7844|TURNER| SALESMAN|7698|2001-09-08|1500.0|   0.0|    30|\n",
      "| 7900| JAMES|    CLERK|7698|2001-12-03| 950.0|  null|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|2001-02-22|1250.0| 500.0|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|2001-02-03|3000.0|  null|    20|\n",
      "| 7369| SMITH|    CLERK|7902|2000-12-17| 800.0|  null|    20|\n",
      "| 7788| SCOTT|  ANALYST|7566|2007-04-19|3000.0|  null|    20|\n",
      "| 7876| ADAMS|    CLERK|7788|2007-05-23|1100.0|  null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|2002-01-23|1300.0|  null|    10|\n",
      "+-----+------+---------+----+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+----------+------+------+------+\n",
      "|empno| ename|     job| mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+--------+----+----------+------+------+------+\n",
      "| 7698| BLAKE| MANAGER|7839|2001-05-01|2850.0|  null|    30|\n",
      "| 7782| CLARK| MANAGER|7839|2001-06-09|2450.0|  null|    10|\n",
      "| 7566| JONES| MANAGER|7839|2001-04-02|2975.0|  null|    20|\n",
      "| 7654|MARTIN|SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|\n",
      "| 7499| ALLEN|SALESMAN|7698|2001-02-20|1600.0| 300.0|    30|\n",
      "| 7844|TURNER|SALESMAN|7698|2001-09-08|1500.0|   0.0|    30|\n",
      "| 7900| JAMES|   CLERK|7698|2001-12-03| 950.0|  null|    30|\n",
      "| 7521|  WARD|SALESMAN|7698|2001-02-22|1250.0| 500.0|    30|\n",
      "| 7902|  FORD| ANALYST|7566|2001-02-03|3000.0|  null|    20|\n",
      "| 7369| SMITH|   CLERK|7902|2000-12-17| 800.0|  null|    20|\n",
      "| 7788| SCOTT| ANALYST|7566|2007-04-19|3000.0|  null|    20|\n",
      "| 7876| ADAMS|   CLERK|7788|2007-05-23|1100.0|  null|    20|\n",
      "| 7934|MILLER|   CLERK|7782|2002-01-23|1300.0|  null|    10|\n",
      "+-----+------+--------+----+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df02.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Replace NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df02 = df.fillna({'comm':0,'mgr': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+------+------+\n",
      "|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+---------+----+----------+------+------+------+\n",
      "| 7839|  KING|PRESIDENT|   0|2001-11-17|5000.0|   0.0|    10|\n",
      "| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0|   0.0|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0|   0.0|    10|\n",
      "| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0|   0.0|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|\n",
      "| 7499| ALLEN| SALESMAN|7698|2001-02-20|1600.0| 300.0|    30|\n",
      "| 7844|TURNER| SALESMAN|7698|2001-09-08|1500.0|   0.0|    30|\n",
      "| 7900| JAMES|    CLERK|7698|2001-12-03| 950.0|   0.0|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|2001-02-22|1250.0| 500.0|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|2001-02-03|3000.0|   0.0|    20|\n",
      "| 7369| SMITH|    CLERK|7902|2000-12-17| 800.0|   0.0|    20|\n",
      "| 7788| SCOTT|  ANALYST|7566|2007-04-19|3000.0|   0.0|    20|\n",
      "| 7876| ADAMS|    CLERK|7788|2007-05-23|1100.0|   0.0|    20|\n",
      "| 7934|MILLER|    CLERK|7782|2002-01-23|1300.0|   0.0|    10|\n",
      "+-----+------+---------+----+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df02.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df02 = df.dropna(how='all', subset =['comm', 'mgr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+----------+------+------+------+\n",
      "|empno| ename|     job| mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+--------+----+----------+------+------+------+\n",
      "| 7698| BLAKE| MANAGER|7839|2001-05-01|2850.0|  null|    30|\n",
      "| 7782| CLARK| MANAGER|7839|2001-06-09|2450.0|  null|    10|\n",
      "| 7566| JONES| MANAGER|7839|2001-04-02|2975.0|  null|    20|\n",
      "| 7654|MARTIN|SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|\n",
      "| 7499| ALLEN|SALESMAN|7698|2001-02-20|1600.0| 300.0|    30|\n",
      "| 7844|TURNER|SALESMAN|7698|2001-09-08|1500.0|   0.0|    30|\n",
      "| 7900| JAMES|   CLERK|7698|2001-12-03| 950.0|  null|    30|\n",
      "| 7521|  WARD|SALESMAN|7698|2001-02-22|1250.0| 500.0|    30|\n",
      "| 7902|  FORD| ANALYST|7566|2001-02-03|3000.0|  null|    20|\n",
      "| 7369| SMITH|   CLERK|7902|2000-12-17| 800.0|  null|    20|\n",
      "| 7788| SCOTT| ANALYST|7566|2007-04-19|3000.0|  null|    20|\n",
      "| 7876| ADAMS|   CLERK|7788|2007-05-23|1100.0|  null|    20|\n",
      "| 7934|MILLER|   CLERK|7782|2002-01-23|1300.0|  null|    10|\n",
      "+-----+------+--------+----+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df02.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550.0\n"
     ]
    }
   ],
   "source": [
    "avg = df.select(mean(df.comm)).collect()[0][0]\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+------+------+\n",
      "|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+---------+----+----------+------+------+------+\n",
      "| 7839|  KING|PRESIDENT|null|2001-11-17|5000.0| 550.0|    10|\n",
      "| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0| 550.0|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0| 550.0|    10|\n",
      "| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0| 550.0|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|\n",
      "| 7499| ALLEN| SALESMAN|7698|2001-02-20|1600.0| 300.0|    30|\n",
      "| 7844|TURNER| SALESMAN|7698|2001-09-08|1500.0|   0.0|    30|\n",
      "| 7900| JAMES|    CLERK|7698|2001-12-03| 950.0| 550.0|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|2001-02-22|1250.0| 500.0|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|2001-02-03|3000.0| 550.0|    20|\n",
      "| 7369| SMITH|    CLERK|7902|2000-12-17| 800.0| 550.0|    20|\n",
      "| 7788| SCOTT|  ANALYST|7566|2007-04-19|3000.0| 550.0|    20|\n",
      "| 7876| ADAMS|    CLERK|7788|2007-05-23|1100.0| 550.0|    20|\n",
      "| 7934|MILLER|    CLERK|7782|2002-01-23|1300.0| 550.0|    10|\n",
      "+-----+------+---------+----+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df02 = df.fillna({'comm':avg})\n",
    "df02.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Datetime Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, dayofyear, weekofyear, hour, minute                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+------+---------+-------+------+----------+----------+\n",
      "|  hiredate|dt_year|dt_month|dt_day|dt_dayofy|dt_hour|dt_min|dt_week_no|    dt_int|\n",
      "+----------+-------+--------+------+---------+-------+------+----------+----------+\n",
      "|2001-11-17|   2001|      11|    17|      321|      0|     0|        46|1005937200|\n",
      "|2001-05-01|   2001|       5|     1|      121|      0|     0|        18| 988657200|\n",
      "|2001-06-09|   2001|       6|     9|      160|      0|     0|        23| 992026800|\n",
      "|2001-04-02|   2001|       4|     2|       92|      0|     0|        14| 986151600|\n",
      "|2001-09-28|   2001|       9|    28|      271|      0|     0|        39|1001617200|\n",
      "|2001-02-20|   2001|       2|    20|       51|      0|     0|         8| 982609200|\n",
      "|2001-09-08|   2001|       9|     8|      251|      0|     0|        36| 999889200|\n",
      "|2001-12-03|   2001|      12|     3|      337|      0|     0|        49|1007319600|\n",
      "|2001-02-22|   2001|       2|    22|       53|      0|     0|         8| 982782000|\n",
      "|2001-02-03|   2001|       2|     3|       34|      0|     0|         5| 981140400|\n",
      "|2000-12-17|   2000|      12|    17|      352|      0|     0|        50| 976993200|\n",
      "|2007-04-19|   2007|       4|    19|      109|      0|     0|        16|1176922800|\n",
      "|2007-05-23|   2007|       5|    23|      143|      0|     0|        21|1179860400|\n",
      "|2002-01-23|   2002|       1|    23|       23|      0|     0|         4|1011726000|\n",
      "+----------+-------+--------+------+---------+-------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df.select(df.hiredate, year(df.hiredate).alias('dt_year'), \\\n",
    "                month(df.hiredate).alias('dt_month'), \\\n",
    "                dayofmonth(df.hiredate).alias('dt_day'), \\\n",
    "                dayofyear(df.hiredate).alias('dt_dayofy'), \\\n",
    "                hour(df.hiredate).alias('dt_hour'), \\\n",
    "                minute(df.hiredate).alias('dt_min'), \\\n",
    "                weekofyear(df.hiredate).alias('dt_week_no'), \\\n",
    "                unix_timestamp(df.hiredate).alias('dt_int'))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. \tFilter Data Based on Conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+------+----+------+\n",
      "|empno|ename|      job| mgr|  hiredate|   sal|comm|deptno|\n",
      "+-----+-----+---------+----+----------+------+----+------+\n",
      "| 7839| KING|PRESIDENT|null|2001-11-17|5000.0|null|    10|\n",
      "| 7782|CLARK|  MANAGER|7839|2001-06-09|2450.0|null|    10|\n",
      "| 7566|JONES|  MANAGER|7839|2001-04-02|2975.0|null|    20|\n",
      "| 7902| FORD|  ANALYST|7566|2001-02-03|3000.0|null|    20|\n",
      "| 7369|SMITH|    CLERK|7902|2000-12-17| 800.0|null|    20|\n",
      "+-----+-----+---------+----+----------+------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((df.deptno == 10) | (df.deptno == 20)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+----------+------+----+------+\n",
      "|empno|ename|      job| mgr|  hiredate|   sal|comm|deptno|\n",
      "+-----+-----+---------+----+----------+------+----+------+\n",
      "| 7839| KING|PRESIDENT|null|2001-11-17|5000.0|null|    10|\n",
      "| 7782|CLARK|  MANAGER|7839|2001-06-09|2450.0|null|    10|\n",
      "| 7566|JONES|  MANAGER|7839|2001-04-02|2975.0|null|    20|\n",
      "| 7902| FORD|  ANALYST|7566|2001-02-03|3000.0|null|    20|\n",
      "| 7369|SMITH|    CLERK|7902|2000-12-17| 800.0|null|    20|\n",
      "+-----+-----+---------+----+----------+------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df.deptno.isin(10, 20)].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|empno| ename|deptno|\n",
      "+-----+------+------+\n",
      "| 7839|  KING|    10|\n",
      "| 7782| CLARK|    10|\n",
      "| 7566| JONES|    20|\n",
      "| 7902|  FORD|    20|\n",
      "| 7369| SMITH|    20|\n",
      "| 7788| SCOTT|    20|\n",
      "| 7876| ADAMS|    20|\n",
      "| 7934|MILLER|    10|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('empno', 'ename', 'deptno').filter((df.deptno == 10) | (df.deptno == 20)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+------+------+\n",
      "|empno|ename|      job|   sal|deptno|\n",
      "+-----+-----+---------+------+------+\n",
      "| 7839| KING|PRESIDENT|5000.0|    10|\n",
      "| 7698|BLAKE|  MANAGER|2850.0|    30|\n",
      "| 7782|CLARK|  MANAGER|2450.0|    10|\n",
      "| 7566|JONES|  MANAGER|2975.0|    20|\n",
      "+-----+-----+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('empno', 'ename', 'job', 'sal', 'deptno').filter((df.sal >2000) & (df.job != 'ANALYST')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\tGroup By with Aggregation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common aggreagation functions for both pandas and pyspark include: sum(), count(),mean(), min(),max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|deptno|sum(sal)|\n",
      "+------+--------+\n",
      "|    20| 10875.0|\n",
      "|    10|  8750.0|\n",
      "|    30|  9400.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(df.deptno).agg(F.sum('sal')).show() # SELECT deptno, SUM(sal) FROM emp GROUP BY deptno\n",
    "#F.mean(), F.max(), F.countDistinct(), F.min(), F.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|deptno|count(job)|\n",
      "+------+----------+\n",
      "|    20|         3|\n",
      "|    10|         3|\n",
      "|    30|         3|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(df.deptno).agg(F.countDistinct('job')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|deptno|count(job)|\n",
      "+------+----------+\n",
      "|    20|         5|\n",
      "|    10|         3|\n",
      "|    30|         6|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(df.deptno).agg(F.count('job')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------------+--------+\n",
      "|deptno|      job|count(empno)|sum(sal)|\n",
      "+------+---------+------------+--------+\n",
      "|    20|  ANALYST|           2|  6000.0|\n",
      "|    20|  MANAGER|           1|  2975.0|\n",
      "|    30|  MANAGER|           1|  2850.0|\n",
      "|    30| SALESMAN|           4|  5600.0|\n",
      "|    30|    CLERK|           1|   950.0|\n",
      "|    10|PRESIDENT|           1|  5000.0|\n",
      "|    20|    CLERK|           2|  1900.0|\n",
      "|    10|    CLERK|           1|  1300.0|\n",
      "|    10|  MANAGER|           1|  2450.0|\n",
      "+------+---------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(['deptno','job']).agg({'sal': 'sum', 'empno': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to compare the aggregation results, since the pandas dataframe and pyspark dataframe are in different orders. The following shows how can we sort the data frame based on specific columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sort Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we use sort_values(), while we use sort() in pyspark to sort the data frame based on specific columns. The default sorting order is ascending.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------------+--------+\n",
      "|deptno|      job|count(empno)|sum(sal)|\n",
      "+------+---------+------------+--------+\n",
      "|    10|    CLERK|           1|  1300.0|\n",
      "|    10|  MANAGER|           1|  2450.0|\n",
      "|    10|PRESIDENT|           1|  5000.0|\n",
      "|    20|  ANALYST|           2|  6000.0|\n",
      "|    20|    CLERK|           2|  1900.0|\n",
      "|    20|  MANAGER|           1|  2975.0|\n",
      "|    30|    CLERK|           1|   950.0|\n",
      "|    30|  MANAGER|           1|  2850.0|\n",
      "|    30| SALESMAN|           4|  5600.0|\n",
      "+------+---------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg = df.groupBy(['deptno','job']).agg({'sal': 'sum', 'empno': 'count'}) \\\n",
    "                                    .sort(['deptno','job'], ascending =True)\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Rename Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the aggregation functions, the names of some columns are not reasonable. We need to rename these column names to avoid confusion. The following shows how can we rename columns in pandas and pyspark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------+\n",
      "|deptno|      job|empCount|salSum|\n",
      "+------+---------+--------+------+\n",
      "|    10|    CLERK|       1|1300.0|\n",
      "|    10|  MANAGER|       1|2450.0|\n",
      "|    10|PRESIDENT|       1|5000.0|\n",
      "|    20|  ANALYST|       2|6000.0|\n",
      "|    20|    CLERK|       2|1900.0|\n",
      "|    20|  MANAGER|       1|2975.0|\n",
      "|    30|    CLERK|       1| 950.0|\n",
      "|    30|  MANAGER|       1|2850.0|\n",
      "|    30| SALESMAN|       4|5600.0|\n",
      "+------+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg = df_agg.withColumnRenamed(\"count(empno)\",\"empCount\").withColumnRenamed(\"sum(sal)\", \"salSum\")\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Create a New Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+------+------+---------+\n",
      "|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|AnnualSal|\n",
      "+-----+------+---------+----+----------+------+------+------+---------+\n",
      "| 7839|  KING|PRESIDENT|null|2001-11-17|5000.0|  null|    10|  60000.0|\n",
      "| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0|  null|    30|  34200.0|\n",
      "| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0|  null|    10|  29400.0|\n",
      "| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0|  null|    20|  35700.0|\n",
      "| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|  15000.0|\n",
      "| 7499| ALLEN| SALESMAN|7698|2001-02-20|1600.0| 300.0|    30|  19200.0|\n",
      "| 7844|TURNER| SALESMAN|7698|2001-09-08|1500.0|   0.0|    30|  18000.0|\n",
      "| 7900| JAMES|    CLERK|7698|2001-12-03| 950.0|  null|    30|  11400.0|\n",
      "| 7521|  WARD| SALESMAN|7698|2001-02-22|1250.0| 500.0|    30|  15000.0|\n",
      "| 7902|  FORD|  ANALYST|7566|2001-02-03|3000.0|  null|    20|  36000.0|\n",
      "| 7369| SMITH|    CLERK|7902|2000-12-17| 800.0|  null|    20|   9600.0|\n",
      "| 7788| SCOTT|  ANALYST|7566|2007-04-19|3000.0|  null|    20|  36000.0|\n",
      "| 7876| ADAMS|    CLERK|7788|2007-05-23|1100.0|  null|    20|  13200.0|\n",
      "| 7934|MILLER|    CLERK|7782|2002-01-23|1300.0|  null|    10|  15600.0|\n",
      "+-----+------+---------+----+----------+------+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.withColumn('AnnualSal', df.sal*12)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Join Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableA =  df\n",
    "TableB = spark.read.csv('D:/DataSets/dept.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableB = TableB.withColumnRenamed(\"deptno\",\"dno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = TableA.alias('e') \n",
    "tb = TableB.alias('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----+------+---------+----+----------+------+------+------+\n",
      "|dno|     dname|     loc|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|\n",
      "+---+----------+--------+-----+------+---------+----+----------+------+------+------+\n",
      "| 10|ACCOUNTING|NEW YORK| 7839|  KING|PRESIDENT|null|2001-11-17|5000.0|  null|    10|\n",
      "| 30|     SALES| CHICAGO| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0|  null|    30|\n",
      "| 10|ACCOUNTING|NEW YORK| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0|  null|    10|\n",
      "| 20|  RESEARCH|  DALLAS| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0|  null|    20|\n",
      "| 30|     SALES| CHICAGO| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|\n",
      "| 30|     SALES| CHICAGO| 7499| ALLEN| SALESMAN|7698|2001-02-20|1600.0| 300.0|    30|\n",
      "| 30|     SALES| CHICAGO| 7844|TURNER| SALESMAN|7698|2001-09-08|1500.0|   0.0|    30|\n",
      "| 30|     SALES| CHICAGO| 7900| JAMES|    CLERK|7698|2001-12-03| 950.0|  null|    30|\n",
      "| 30|     SALES| CHICAGO| 7521|  WARD| SALESMAN|7698|2001-02-22|1250.0| 500.0|    30|\n",
      "| 20|  RESEARCH|  DALLAS| 7902|  FORD|  ANALYST|7566|2001-02-03|3000.0|  null|    20|\n",
      "| 20|  RESEARCH|  DALLAS| 7369| SMITH|    CLERK|7902|2000-12-17| 800.0|  null|    20|\n",
      "| 20|  RESEARCH|  DALLAS| 7788| SCOTT|  ANALYST|7566|2007-04-19|3000.0|  null|    20|\n",
      "| 20|  RESEARCH|  DALLAS| 7876| ADAMS|    CLERK|7788|2007-05-23|1100.0|  null|    20|\n",
      "| 10|ACCOUNTING|NEW YORK| 7934|MILLER|    CLERK|7782|2002-01-23|1300.0|  null|    10|\n",
      "+---+----------+--------+-----+------+---------+----+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.join(ta, ta.deptno==tb.dno).show()\n",
    "# right, right_outer, full, default ‘inner’, how ='left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+\n",
      "|dno|     dname|     loc|\n",
      "+---+----------+--------+\n",
      "| 10|ACCOUNTING|NEW YORK|\n",
      "| 20|  RESEARCH|  DALLAS|\n",
      "| 30|     SALES| CHICAGO|\n",
      "| 40|OPERATIONS|  BOSTON|\n",
      "+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TableB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----+------+---------+----+----------+------+------+------+\n",
      "|dno|     dname|     loc|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|\n",
      "+---+----------+--------+-----+------+---------+----+----------+------+------+------+\n",
      "| 10|ACCOUNTING|NEW YORK| 7934|MILLER|    CLERK|7782|2002-01-23|1300.0|  null|    10|\n",
      "| 10|ACCOUNTING|NEW YORK| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0|  null|    10|\n",
      "| 10|ACCOUNTING|NEW YORK| 7839|  KING|PRESIDENT|null|2001-11-17|5000.0|  null|    10|\n",
      "| 20|  RESEARCH|  DALLAS| 7876| ADAMS|    CLERK|7788|2007-05-23|1100.0|  null|    20|\n",
      "| 20|  RESEARCH|  DALLAS| 7788| SCOTT|  ANALYST|7566|2007-04-19|3000.0|  null|    20|\n",
      "| 20|  RESEARCH|  DALLAS| 7369| SMITH|    CLERK|7902|2000-12-17| 800.0|  null|    20|\n",
      "| 20|  RESEARCH|  DALLAS| 7902|  FORD|  ANALYST|7566|2001-02-03|3000.0|  null|    20|\n",
      "| 20|  RESEARCH|  DALLAS| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0|  null|    20|\n",
      "| 30|     SALES| CHICAGO| 7521|  WARD| SALESMAN|7698|2001-02-22|1250.0| 500.0|    30|\n",
      "| 30|     SALES| CHICAGO| 7900| JAMES|    CLERK|7698|2001-12-03| 950.0|  null|    30|\n",
      "| 30|     SALES| CHICAGO| 7844|TURNER| SALESMAN|7698|2001-09-08|1500.0|   0.0|    30|\n",
      "| 30|     SALES| CHICAGO| 7499| ALLEN| SALESMAN|7698|2001-02-20|1600.0| 300.0|    30|\n",
      "| 30|     SALES| CHICAGO| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30|\n",
      "| 30|     SALES| CHICAGO| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0|  null|    30|\n",
      "| 40|OPERATIONS|  BOSTON| null|  null|     null|null|      null|  null|  null|  null|\n",
      "+---+----------+--------+-----+------+---------+----+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb.join(ta, ta.deptno==tb.dno, how = 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+------+------+---+----------+--------+\n",
      "|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|dno|     dname|     loc|\n",
      "+-----+------+---------+----+----------+------+------+------+---+----------+--------+\n",
      "| 7934|MILLER|    CLERK|7782|2002-01-23|1300.0|  null|    10| 10|ACCOUNTING|NEW YORK|\n",
      "| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0|  null|    10| 10|ACCOUNTING|NEW YORK|\n",
      "| 7839|  KING|PRESIDENT|null|2001-11-17|5000.0|  null|    10| 10|ACCOUNTING|NEW YORK|\n",
      "| 7876| ADAMS|    CLERK|7788|2007-05-23|1100.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7788| SCOTT|  ANALYST|7566|2007-04-19|3000.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7369| SMITH|    CLERK|7902|2000-12-17| 800.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7902|  FORD|  ANALYST|7566|2001-02-03|3000.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7521|  WARD| SALESMAN|7698|2001-02-22|1250.0| 500.0|    30| 30|     SALES| CHICAGO|\n",
      "| 7900| JAMES|    CLERK|7698|2001-12-03| 950.0|  null|    30| 30|     SALES| CHICAGO|\n",
      "| 7844|TURNER| SALESMAN|7698|2001-09-08|1500.0|   0.0|    30| 30|     SALES| CHICAGO|\n",
      "| 7499| ALLEN| SALESMAN|7698|2001-02-20|1600.0| 300.0|    30| 30|     SALES| CHICAGO|\n",
      "| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30| 30|     SALES| CHICAGO|\n",
      "| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0|  null|    30| 30|     SALES| CHICAGO|\n",
      "| null|  null|     null|null|      null|  null|  null|  null| 40|OPERATIONS|  BOSTON|\n",
      "+-----+------+---------+----+----------+------+------+------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ta.join(tb, ta.deptno==tb.dno, how = 'right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+------+------+------+---+----------+--------+\n",
      "|empno| ename|      job| mgr|  hiredate|   sal|  comm|deptno|dno|     dname|     loc|\n",
      "+-----+------+---------+----+----------+------+------+------+---+----------+--------+\n",
      "| 7566| JONES|  MANAGER|7839|2001-04-02|2975.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7902|  FORD|  ANALYST|7566|2001-02-03|3000.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7369| SMITH|    CLERK|7902|2000-12-17| 800.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7788| SCOTT|  ANALYST|7566|2007-04-19|3000.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| 7876| ADAMS|    CLERK|7788|2007-05-23|1100.0|  null|    20| 20|  RESEARCH|  DALLAS|\n",
      "| null|  null|     null|null|      null|  null|  null|  null| 40|OPERATIONS|  BOSTON|\n",
      "| 7839|  KING|PRESIDENT|null|2001-11-17|5000.0|  null|    10| 10|ACCOUNTING|NEW YORK|\n",
      "| 7782| CLARK|  MANAGER|7839|2001-06-09|2450.0|  null|    10| 10|ACCOUNTING|NEW YORK|\n",
      "| 7934|MILLER|    CLERK|7782|2002-01-23|1300.0|  null|    10| 10|ACCOUNTING|NEW YORK|\n",
      "| 7698| BLAKE|  MANAGER|7839|2001-05-01|2850.0|  null|    30| 30|     SALES| CHICAGO|\n",
      "| 7654|MARTIN| SALESMAN|7698|2001-09-28|1250.0|1400.0|    30| 30|     SALES| CHICAGO|\n",
      "| 7499| ALLEN| SALESMAN|7698|2001-02-20|1600.0| 300.0|    30| 30|     SALES| CHICAGO|\n",
      "| 7844|TURNER| SALESMAN|7698|2001-09-08|1500.0|   0.0|    30| 30|     SALES| CHICAGO|\n",
      "| 7900| JAMES|    CLERK|7698|2001-12-03| 950.0|  null|    30| 30|     SALES| CHICAGO|\n",
      "| 7521|  WARD| SALESMAN|7698|2001-02-22|1250.0| 500.0|    30| 30|     SALES| CHICAGO|\n",
      "+-----+------+---------+----+----------+------+------+------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ta.join(tb, ta.deptno==tb.dno, how = 'full').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
